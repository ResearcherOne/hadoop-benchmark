# SWIM

Benchmark based on [SWIM](https://github.com/SWIMProjectUCB/SWIM).

## Image

In this example, we generated a simple concurrent workload which only consists of 50 Mapreduce jobs.
The workloads files are visible under `image/SWIM` directory.

Users can also generate new Mapreduce workloads by following [SWIM tutorial](https://github.com/SWIMProjectUCB/SWIM/wiki/Analyze-historical-cluster-traces-and-synthesize-representative-workload), and only replace the workloads files under `image/SWIM` with those of new workloads.

PS: To facilitate users to collect the log files of SWIM, we modified `run-all-jobs.sh`.
When users replace the example by new workloads, please add the bellow code to the end of `run-all-jobs.sh` file.

```sh
for job in `jobs -p`
do
  echo "Waiting for $job to finish"
  wait $job
done

logs="workGenLogs-$(date +"%Y%m%d-%H%M").tgz"
tar cfvz "$logs" "$SWIM_HOME/workGenLogs"
hdfs dfs -put "$logs" "/user/root/$logs"

echo "Benchmarks finished"
echo "Logs uploaded to HDFS: /user/root/$logs"
```

## Analysis

In `analysis` directory, we provide a simple R script to help users analyse the log files and generate graphs.

After users have collected all job logs from hadoop-benchmark, pleas put all of them in proper directories:
- `results/swim/vanilla` directory is to store all the job logs generated by vanilla Hadoop cluster
- `results/swim/sa` directory is used for storing the job logs captured in Self-adaptation scenario

And then, the R command to analysis the log files should be like:
```sh
	$ Rscript swim-report.R results/swim/
```