<!DOCTYPE html>
<html>
<head>
<h1>SEAMS16</h1>
</head>
<body>
<h2>Hadoop-Benchmark: Rapid Prototyping and Evaluation of Self-Adaptive Behaviors in Hadoop Clusters</h2>

<h2>1. Introduction</h2>

This is a snapshot of the github project, for the latest version and additional resources, please visit https://github.com/Spirals-Team/hadoop-benchmark.
<br/>
<br/>

Hadoop is a famous software to achieve reliable, scalable and distributed computing.
But setting a Hadoop cluster always troubles users by its configurations and networking.
Moreover, in research community, there is no an easy way to quickly reproduce a running Hadoop cluster which can help researchers compare different existing approaches.
<br/>
<br/>
This project use Docker to package a Hadoop cluster and all its dependencies in images.
Users can use Docker containers to quickly build an Hadoop infrastructure.
We also provide some acknowledged benchmarks and self-adaptive scenarios atop of it.
<br/>
<br/>
The source code of this project is all stored in <a href="https://github.com/Spirals-Team/hadoop-benchmark">GitHub</a>.
<br/>
<br/>




The main directory of this project contains several important components:<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- cluster.sh<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- images<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- benchmarks<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- scenarios<br/>
<br/>

<h4>cluster.sh</h4>
 - It is the main bash of this project. The details can be found by command:<br/><br/> 

&nbsp;&nbsp;&nbsp;&nbsp;$ ./cluster.sh --help<br/><br/>

 - With the default environment value, this bash will create an example Hadoop cluster which is composed by three nodes:<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- Consul K/V store<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- Hadoop control node<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- Hadoop compute node 1<br/>
&nbsp;&nbsp;&nbsp;&nbsp;(Consul K/V store is used by docker to create an overlay network which help docker containers in different hosts to connect to each other.)
 
<h4>images</h4>
  - This directory contains the Dockerfiles of Hadoop images. The built images should package the compiled Hadoop code, prepared configuration files and all required dependencies.
 
<h4>benchmarks</h4>
  - This directory provide 3 benchmarks.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- The bundled Hadoop examples<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- Hibench<br/>
&nbsp;&nbsp;&nbsp;&nbsp;- SWIM - default 50 jobs <br/>
	
<h4>scenarios</h4>
 - This directory provide the source code of an alternative Hadoop images. Besides the basic Hadoop environment, a self-adaptive approach is also packaged in these images.








<h2>2. Requirements</h2>

This project is based on Docker and Bash script.
Before the start, please ensure that the below tools have been well installed.
The links following the software is the official tutorial or commands of installation.

<h4>docker (version >= 1.9.1)</h4>
<h5><a href="https://docs.docker.com/engine/installation/">Linux</a></h5>
<h5><a href="https://docs.docker.com/engine/installation/mac/">Mac</a></h5>
<h5><a href="https://docs.docker.com/engine/installation/windows/">windows</a></h5>

<h4>docker-machine (version >= 0.5.6)</h4>
<h5>Linux</h5>
&nbsp;&nbsp;&nbsp;&nbsp;$ curl -L https://github.com/docker/machine/releases/download/v0.5.6/docker-machine_linux-amd64 >/usr/local/bin/docker-machine && \
    chmod +x /usr/local/bin/docker-machine
<h5>Mac</h5>
&nbsp;&nbsp;&nbsp;&nbsp;$ curl -L https://github.com/docker/machine/releases/download/v0.5.6/docker-machine_darwin-amd64 >/usr/local/bin/docker-machine && \
    chmod +x /usr/local/bin/docker-machine
<h5>windows</h5>
&nbsp;&nbsp;&nbsp;&nbsp;$ if [[ ! -d "$HOME/bin" ]]; then mkdir -p "$HOME/bin"; fi && \
    curl -L https://github.com/docker/machine/releases/download/v0.5.6/docker-machine_windows-amd64.exe > "$HOME/bin/docker-machine.exe" && \
    chmod +x "$HOME/bin/docker-machine.exe"


<h4>Bash (version >= 3)</h4>


<h4>Option: Project download</h4>
This project can be also cloned from Github.<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ git clone https://github.com/Spirals-Team/hadoop-benchmark.git




<h2>3. Getting started guide</h2>
The next section is simplified "Getting started guide" without descriptions.

<h3>3.0 Configuration</h3>
Open the 'local_cluster' file and modify the desired number of nodes, the 'NUM_COMPUTE_NODES' settings.<br/><br/>
The default deployment is on local machine using <a href="https://www.virtualbox.org/">ORACLE VirtualBox</a>. One hadoop instance requires about 2GB of RAM so be rather conservative to how many nodes can fit to your machine.<br/>
A number of additional virtualization environments are support.
The list, including all theior settings can be found at the <a href="https://docs.docker.com/machine/drivers/">docker-machine website</a>.<br/><br/>
To use different driver, simply change the 'DRIVER' variable export all require properties based on the driver requirements. 

<h3>3.1 Creating a cluster</h3>

When the project is downloaded, users can create an example Hadoop cluster by one command:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh create-cluster<br/>
<br/>

By modifying 'NUM_COMPUTE_NODES', users can create a different scale Hadoop cluster by the above command.
<br/>
<br/>
By default, this command will create the consul K/V store node firstly.
And then, a consul K/V store container will be launched in this node.
<br/>
<br/> 
After the consul K/V store is ready, two nodes will be created as Hadoop control node and compute node 1.
<br/>
The command will create an overlay network for all docker containers in this cluster at the end.
<br/>
Users can use the following command to check the status of the nodes created:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh status-cluster<br/>
<br/>

<h3>3.2 Starting hadoop</h3> 
Once the step one finished, users can start hadoop cluster by another command:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh start-hadoop<br/>
<br/>
 
This command will create a Hadoop control container in Hadoop control node.
This container shall run ResourceManager, NameNode, SecondaryNamenode and JobHistoryServer which are master components of Hadoop.
<br/>
<br/>
When the Hadoop control container is running, a Hadoop compute container will be launched in Hadoop Compute node 1.
This container supports NodeManager and Datanode which are slave agent of Hadoop.

<h3>3.3 Running bechnmarks</h3>
After step two successes, users can execute different benchmarks in the running Hadoop cluster.
 
<h4>3.3.1 Quick test with Hadoop bundled examples</h4>
Users can quickly test the Hadoop cluster with the bundled example Hadoop commands.
For example:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ ./benchmarks/hadoop-mapreduce-examples/run.sh pi 2 2<br/>
<br/>
 
<h4>3.3.2 Run HiBench</h4>
Users can also run HiBench on the Hadoop cluster, which is a famous hadoop benchmark provided by Intel. The launch command is like:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ ./benchmarks/hibench/run.sh<br/>
<br/>
 (Warning: each HiBench command will generates a lot of data (e.g. terasort input data is 1TB). So HiBench  is not suitable for a local machine.)
 
<h4>3.3.3 Run SWIM</h4>
 In this project, a SWIM example workloads is also provided.
 This concurrent scenario contains 50 concurrent MapReduce jobs.
 Users can launch this benchmark by the below command:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ ./benchmarks/swim/run.sh<br/>
<br/>
At the end of the test, all the job logs is stored in the directory "workGenLogs" in current directory.


<h2>4. Step-by-Step Instructions</h2>

<h4>&nbsp;&nbsp;&nbsp;&nbsp;Download the project</h4>
&nbsp;&nbsp;&nbsp;&nbsp;$ git clone https://github.com/Spirals-Team/hadoop-benchmark.git

<h4>&nbsp;&nbsp;&nbsp;&nbsp;Start the cluster</h4>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh create-cluster<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh status-cluster<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh start-hadoop<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ eval $(docker-machine env --swarm local-hadoop-controller)

<h4>&nbsp;&nbsp;&nbsp;&nbsp;Quickly test the cluster</h4>
&nbsp;&nbsp;&nbsp;&nbsp;$ ./benchmarks/hadoop-mapreduce-examples/run.sh pi 2 2<br/>


<h2>5. Self-balancing Scenario</h2>
Before this scenario, please ensure the Hadoop cluster has been stopped.<br/>
Users can stop the Hadoop cluster by command:
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=local_cluster ./cluster.sh stop-hadoop<br/>
<br/>
 
In Self-balacing Scenario, besides a running Hadooop cluster, an self-adaptive approach is also running in Hadoop control node.<br/>
This approach automatically balances the job-parallelism and job-throughput based on the memory utilization of the whole Hadoop cluster.
 
To start Self-balancing Scenario, all commands used are similar to those in Section 3.<br/>
But the 'local_cluster' configuration file should be replaced with the 'self-balancing-example/local_cluster' file.<br/>
<br/>
<br/>
The Self-balancing Scenario Step-by-Step Instructions:
<br/><br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=scenarios/self-balancing-example/local_cluster ./cluster.sh start-hadoop<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=scenarios/self-balancing-example/local_cluster ./cluster.sh status-cluster<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ CONFIG=scenarios/self-balancing-example/local_cluster ./cluster.sh start-hadoop<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ eval $(docker-machine env --swarm local-hadoop-controller)
<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;$ ./benchmarks/hadoop-mapreduce-examples/run.sh pi 2 2<br/>







</body>
</html>

